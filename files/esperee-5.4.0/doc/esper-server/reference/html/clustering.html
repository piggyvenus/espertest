<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title xmlns:d="http://docbook.org/ns/docbook" xmlns:rf="java:org.jboss.highlight.XhtmlRendererFactory">Chapter 4. Clustering, Scalability and High-Availability</title><link rel="stylesheet" href="css/espertech.css" type="text/css"/><meta xmlns:d="http://docbook.org/ns/docbook" xmlns:rf="java:org.jboss.highlight.XhtmlRendererFactory" name="generator" content="DocBook XSL-NS Stylesheets V1.74.0"/><meta xmlns:d="http://docbook.org/ns/docbook" xmlns:rf="java:org.jboss.highlight.XhtmlRendererFactory" http-equiv="Content-Type" content="text/html; charset=UTF-8"/><link rel="home" href="index.html" title="Esper Enterprise Edition - Server"/><link rel="up" href="index.html" title="Esper Enterprise Edition - Server"/><link rel="prev" href="architecture.html" title="Chapter 3. Architecture Overview"/><link rel="next" href="hotdeploy.html" title="Chapter 5. Hot Deployment of CEP Applications and CEP Engines"/></head><body><p xmlns:d="http://docbook.org/ns/docbook" id="title"><a href="http://www.espertech.com" class="site_href"><strong>www.espertech.com</strong></a><a href="http://www.espertech.com/esper/documentation.php" class="doc_href"><strong>Documentation</strong></a></p><ul xmlns:d="http://docbook.org/ns/docbook" class="docnav"><li class="previous"><a accesskey="p" href="architecture.html"><strong>Prev</strong></a></li><li class="next"><a accesskey="n" href="hotdeploy.html"><strong>Next</strong></a></li></ul><div class="chapter" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="clustering"/>Chapter 4. Clustering, Scalability and High-Availability</h2></div></div></div><div class="toc"><dl><dt><span class="sect1"><a href="clustering.html#clustering-related">4.1. Related Information</a></span></dt><dt><span class="sect1"><a href="clustering.html#clustering-mirror">4.2. Mirrored Hot-Hot Pattern</a></span></dt><dt><span class="sect1"><a href="clustering.html#clustering-hotstandbysharedstorage">4.3. Hot-Standby with Shared Storage Pattern</a></span></dt><dd><dl><dt><span class="sect2"><a href="clustering.html#clustering-hotstandbysharedstorage-disaster">4.3.1. Disaster Recovery Scenario</a></span></dt></dl></dd><dt><span class="sect1"><a href="clustering.html#clustering-hotstandbystateless">4.4. Hot-Standby Stateless Pattern</a></span></dt><dt><span class="sect1"><a href="clustering.html#clustering-partitionedstream">4.5. Partitioned Stream Scalability Pattern</a></span></dt><dt><span class="sect1"><a href="clustering.html#clustering-partitionedusecase">4.6. Partition by Use Case Scalability Pattern</a></span></dt></dl></div><p>
		We use the term clustering to mean multiple Enterprise Edition servers working together to achieve certain goals. For scalability, clustering technologies can be used to add processing power to event processing engines by adding servers that share the load. For high availability, a server cluster can be used to maximize uptime of event processing engines.
    </p><p>
		While Enterprise Edition components have clustering considerations integrated at the product design level, there is no single approach to clustering that is optimal for all use cases and implementation sites. Implementing a cluster is a matter of assembling components in different configurations.
	</p><p>
	   We provide a list of related information and an overview of common clustering patterns next.
	</p><div class="sect1" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="clustering-related"/>4.1. Related Information</h2></div></div></div><p>
			  Please see the Enterprise Edition server documentation (this document) for configuring server failover.
		</p><p>
			Enterprise Edition includes a TCP socket-based transport for server-to-server communication. Please find more information in <a class="xref" href="sockettransport.html" title="Chapter 7. Socket Transport">Chapter 7, <i>Socket Transport</i></a>.
		</p><p>
			  Please see the EsperIO documentation for configuring common server-to-server transports such as AMQP or JMS.
		</p></div><div class="sect1" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="clustering-mirror"/>4.2. Mirrored Hot-Hot Pattern</h2></div></div></div><p>
          In this pattern there are two or more cluster members that both (or each) receive all input.
        </p><p>
          The following diagram illustrates this pattern:
        </p><div class="mediaobject" align="center"><img src="images/esperhq_clustering_mirroredhothot.gif" align="middle"/></div><p>
		  The characteristics of this pattern are:
		</p><div class="itemizedlist"><ul><li><p>There are two servers per cluster and each server has identical EPL statements.</p></li><li><p>Each server receives all input events and performs all calculations.</p></li><li><p>All servers in the cluster are primary servers.</p></li><li><p>The downstream system receives output events from each server (two of everything).</p></li></ul></div></div><div class="sect1" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="clustering-hotstandbysharedstorage"/>4.3. Hot-Standby with Shared Storage Pattern</h2></div></div></div><p>
          In this pattern there are two cluster members that are in a hot-standby relationship. The EsperHA store (file system or db) is accessible to both servers. Only the primary server receives all input. The standby secondary server waits to take over from the primary server.
        </p><p>
          The following diagram illustrates this pattern:
        </p><div class="mediaobject" align="center"><img src="images/esperhq_clustering_hotstandbysharedstorage.gif" align="middle"/></div><p>
		  The characteristics of this pattern are:
		</p><div class="itemizedlist"><ul><li><p>Critical event processing application state is written to a shared storage. This is functionality provided by EsperHA. The shared storage can be a disk, a disk on a storage area network (SAN), a disk on a network-attached storage (NAS) or can be a relational database.</p></li><li><p>The primary server receives all input events and performs all calculations.</p></li><li><p>When the primary server fails the secondary server takes over. This is detected by means of heartbeat or operationally. The secondary server recovers state from shared storage including event processing continuous queries.</p></li></ul></div><div class="sect2" lang="en-US"><div class="titlepage"><div><div><h3 class="title"><a id="clustering-hotstandbysharedstorage-disaster"/>4.3.1. Disaster Recovery Scenario</h3></div></div></div><p>
			  To implement a disaster recover scenario, an offsite implementation can combine the hot-standby with shared storage pattern. 
			  The disaster recovery site can run an identical deployment by means of shared storage implemented over a network connection using either SAN or relational database and replication. 
			</p></div></div><div class="sect1" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="clustering-hotstandbystateless"/>4.4. Hot-Standby Stateless Pattern</h2></div></div></div><p>
          This pattern is easy to set up and most suitable for entirely stateless or nearly stateless event processing. In this pattern there are two cluster members that are in a hot-standby relationship. The two servers do not share storage. Only the primary server receives all input. The standby secondary server waits to take over from the primary server.
        </p><p>
          The following diagram illustrates this pattern:
        </p><div class="mediaobject" align="center"><img src="images/esperhq_clustering_hotstandbystateless.gif" align="middle"/></div><p>
		  The characteristics of this pattern are:
		</p><div class="itemizedlist"><ul><li><p>The primary server receives all input events and performs all calculations.</p></li><li><p>When the primary server fails the secondary server takes over. This is detected by means of heartbeat or operationally.</p></li></ul></div></div><div class="sect1" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="clustering-partitionedstream"/>4.5. Partitioned Stream Scalability Pattern</h2></div></div></div><p>
          This pattern partitions the stream to scale an Enterprise Edition cluster from one to multiple servers. A stream is partitioned among the nodes in the cluster. Each server receives as input events a partition (a subset) of the stream. Operations applied to a stream are applied by each server in parallel across each partition.
        </p><p>
          The following diagram illustrates this pattern:
        </p><div class="mediaobject" align="center"><img src="images/esperhq_clustering_partitionedstream.gif" align="middle"/></div><p>
		  The characteristics of this pattern are:
		</p><div class="itemizedlist"><ul><li><p>Consistent hashing is a common means to partitioning the stream. A second option is assigning individual keys or key ranges to servers.</p></li><li><p>An Esper Enterprise Edition server acts as a dispatcher of input stream events, wherein each input stream event is assigned to a single partition (substreams) and dispatched to servers.</p></li><li><p>Each server executes identical EPL statements on a subset of input stream events.</p></li></ul></div></div><div class="sect1" lang="en-US"><div class="titlepage"><div><div><h2 class="title"><a id="clustering-partitionedusecase"/>4.6. Partition by Use Case Scalability Pattern</h2></div></div></div><p>
          This pattern assigns use cases to servers to scale an Enterprise Edition cluster from one to multiple servers. Each server receives all input stream events and executes a subset of EPL statements.
        </p><p>
          The following diagram illustrates this pattern:
        </p><div class="mediaobject" align="center"><img src="images/esperhq_clustering_partitionedusecase.gif" align="middle"/></div><p>
		  The characteristics of this pattern are:
		</p><div class="itemizedlist"><ul><li><p>Each server receives all input events and performs a subset of EPL statements specific to the use cases assigned to the server.</p></li><li><p>All servers in the cluster are primary servers.</p></li><li><p>The downstream system receives output events from each server (per use case).</p></li></ul></div></div></div><ul xmlns:d="http://docbook.org/ns/docbook" class="docnav"><li class="previous"><a accesskey="p" href="architecture.html"><strong>Prev</strong>Chapter 3. Architecture Overview</a></li><li class="up"><a accesskey="u" href="#"><strong>Top of page</strong></a></li><li class="home"><a accesskey="h" href="index.html"><strong>Front page</strong></a></li><li class="next"><a accesskey="n" href="hotdeploy.html"><strong>Next</strong>Chapter 5. Hot Deployment of CEP Applications and...</a></li></ul></body></html>
